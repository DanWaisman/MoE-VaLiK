# MoE-VaLiK: A More Efficient Approach to Multimodal Knowledge Graph Generation

This repository contains the code for a master's research project that explores the use of a Mix-of-Experts (MoE) approach to improve the computational efficiency of the VaLiK pipeline for Multimodal Knowledge Graph (MMKG) generation.

The traditional VaLiK pipeline generates MMKGs from images by using a fixed sequence of three Visual Language Models (VLMs) in a Chain-of-Experts (CoE) approach. This method is computationally expensive as it processes every image with all three VLMs, regardless of image complexity.

This project introduces MoE-VaLiK, a novel pipeline that replaces the CoE approach with a dynamic MoE mechanism. A custom-designed neural network acts as a router, learning a policy to select the optimal number of VLMs for each image to reduce computational cost while aiming to maintain the quality of the generated MMKG.

---

### Installation

**Installing required dependencies**:

```pip install -r ./requirements.txt```

---

### Usage

**Description Generation**

The first part of the pipeline involves generating descriptions for the images in the chosen dataset. The original VaLiK pipeline carried this out following Chain-of-Experts principles. You complete this step by running the following command in the terminal:

```python ./src/Description_Generation/Image_to_Text_CoE.py.py --input [image_dataset]```

The MoE-VaLiK pipeline carries out this step following Mix-of-Experts principles instead. This step requires a pre-trained neural network, which will act as a router. You can carry out this step by running the command below:

```python ./src/Description_Generation/Image_to_Text_MoE.py --input [image_dataset] --router_path [router_model_path]```

**Similarity Verification**

The second part of the pipeline can be carried out with the command shown below. A similarity threshold, below which sentences will be pruned, will have to be chosen.

```python ./src/Prune/similarity_verification.py --image_dir [image_dataset] --threshold [tau_value] --mode [delimiter_type]```

**Graph Generation**

The last step of the pipeline, in which the MMKG is put together, is carried out with the command shown below.

```python ./src/Graph\ Generation/lightrag_execution.py```

**Router Training**

In the first step, following MoE principles, a router has to be provided. The command below can be used to train set router, where the training data and training parameters have to be defined.

```python ./src/Training_Pipeline/router_training.py --input_dir [image_dataset] --output_dir [trained_router] --epochs [epochs] --batch_size [batch_size] --learning_rate [learning_rate] --lambda_cost [lambda_cost] --dropout_rate [dropout_rate]```

```python ./src/Training_Pipeline/router_training.py --plot_from_csv [path_to_training_log]```

```python ./src/Training_Pipeline/router_training.py --input_dir [image_dataset] --resume_from_checkpoint [path_to_stored_model]```

**Evaluation**:

Finally, the multimodal knowlegde graphs generated through the MoE-VaLiK pipeline, can be evaluated on a Visual Question-Answering (VQA) task, using the commands below. The first carries out a baseline, completing the task with a LLM, without any knowledge graph augmentation.

```python ./src/Evaluation/eval_LLM.py --num_questions [num_questions]```

This second command carries out the VQA using Retrieval Augmented Generation, where the LLM was augmented with the MMKG generated by the MoE-VaLiK method.

```python ./src/Evaluation/eval_MMKGs.py --working_dir [mmkg_directory] --num_questions [num_questions]```

---
